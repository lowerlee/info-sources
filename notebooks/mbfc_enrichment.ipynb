{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# MBFC Enrichment Notebook\n",
    "\n",
    "This notebook enriches information sources with Media Bias Fact Check (MBFC) data by scraping bias and factual reporting ratings.\n",
    "\n",
    "## Purpose\n",
    "Automatically fetch MBFC bias and factual ratings for sources in the Google Sheet and update the sheet with this information.\n",
    "\n",
    "## Requirements\n",
    "- **Credentials**: `credentials.json` file in the root directory (Google service account)\n",
    "- **Dependencies**: beautifulsoup4, requests, google-api-python-client\n",
    "- **Sheet Columns**: The sheet must have `mbfc_bias` and `mbfc_factual` columns\n",
    "\n",
    "## How it works\n",
    "1. Connects to Google Sheets and loads source data\n",
    "2. For each source without MBFC data:\n",
    "   - Searches for the source on mediabiasfactcheck.com\n",
    "   - Extracts bias rating and factual reporting rating\n",
    "   - Updates the Google Sheet with the findings\n",
    "3. Applies rate limiting to avoid overwhelming MBFC servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import re\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SERVICE_ACCOUNT_FILE = \"credentials.json\"\n",
    "SPREADSHEET_ID = \"1NywRL9IBR69R0eSrOE9T6mVUbfJHwaALL0vp2K0TLbY\"\n",
    "SHEET_RANGE = \"main!A:H\"\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "\n",
    "# MBFC Configuration\n",
    "MBFC_BASE_URL = \"https://mediabiasfactcheck.com/\"\n",
    "DELAY_BETWEEN_REQUESTS = 2.0  # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract domain name from URL and remove www prefix.\n",
    "    \n",
    "    Args:\n",
    "        url: Full URL string\n",
    "        \n",
    "    Returns:\n",
    "        Domain name without www prefix\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc or parsed.path\n",
    "        # Remove www prefix\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "        return domain\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def search_mbfc(source_name: str, source_url: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Search for source on MBFC by trying different URL patterns.\n",
    "    \n",
    "    Args:\n",
    "        source_name: Name of the source\n",
    "        source_url: URL of the source\n",
    "        \n",
    "    Returns:\n",
    "        MBFC page URL if found, None otherwise\n",
    "    \"\"\"\n",
    "    # Convert source name to slug format (lowercase, replace spaces with hyphens)\n",
    "    name_slug = source_name.lower().strip()\n",
    "    name_slug = re.sub(r'[^a-z0-9\\s-]', '', name_slug)\n",
    "    name_slug = re.sub(r'\\s+', '-', name_slug)\n",
    "    name_slug = re.sub(r'-+', '-', name_slug)\n",
    "    \n",
    "    # Extract domain from URL\n",
    "    domain = extract_domain(source_url)\n",
    "    domain_slug = domain.replace('.', '-') if domain else \"\"\n",
    "    \n",
    "    # Try different URL patterns\n",
    "    patterns_to_try = []\n",
    "    if name_slug:\n",
    "        patterns_to_try.append(name_slug)\n",
    "    if domain_slug and domain_slug != name_slug:\n",
    "        patterns_to_try.append(domain_slug)\n",
    "    \n",
    "    for pattern in patterns_to_try:\n",
    "        try:\n",
    "            mbfc_url = f\"{MBFC_BASE_URL}{pattern}/\"\n",
    "            response = requests.get(mbfc_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            if response.status_code == 200 and 'Bias Rating:' in response.text:\n",
    "                return mbfc_url\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_mbfc_data(mbfc_url: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Parse MBFC page HTML and extract bias and factual ratings.\n",
    "    \n",
    "    Args:\n",
    "        mbfc_url: URL of the MBFC page\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (bias_rating, factual_rating)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(mbfc_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        if response.status_code != 200:\n",
    "            return None, None\n",
    "        \n",
    "        html_content = response.text\n",
    "        \n",
    "        # Extract bias rating\n",
    "        bias_match = re.search(r'Bias Rating:\\s*([A-Z\\s-]+)', html_content, re.IGNORECASE)\n",
    "        bias_rating = bias_match.group(1).strip() if bias_match else None\n",
    "        \n",
    "        # Extract factual reporting\n",
    "        factual_match = re.search(r'Factual Reporting:\\s*([A-Z\\s-]+)', html_content, re.IGNORECASE)\n",
    "        factual_rating = factual_match.group(1).strip() if factual_match else None\n",
    "        \n",
    "        return bias_rating, factual_rating\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def get_mbfc_ratings(source_name: str, source_url: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Combine search and extraction to get MBFC ratings for a source.\n",
    "    \n",
    "    Args:\n",
    "        source_name: Name of the source\n",
    "        source_url: URL of the source\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (bias_rating, factual_rating)\n",
    "    \"\"\"\n",
    "    mbfc_url = search_mbfc(source_name, source_url)\n",
    "    if mbfc_url:\n",
    "        return extract_mbfc_data(mbfc_url)\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sheet_data():\n",
    "    \"\"\"\n",
    "    Load data from Google Sheets.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (sheets_service, headers, data_rows)\n",
    "    \"\"\"\n",
    "    print(\"üîó Connecting to Google Sheets...\")\n",
    "    creds = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE,\n",
    "        scopes=SCOPES\n",
    "    )\n",
    "    sheets_service = build(\"sheets\", \"v4\", credentials=creds)\n",
    "    print(\"‚úÖ Connected to Google Sheets\")\n",
    "    \n",
    "    print(\"üìÇ Loading data from Google Sheet...\")\n",
    "    sheet = sheets_service.spreadsheets()\n",
    "    result = sheet.values().get(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=SHEET_RANGE\n",
    "    ).execute()\n",
    "    \n",
    "    values = result.get(\"values\", [])\n",
    "    \n",
    "    if not values:\n",
    "        raise ValueError(\"‚ùå No data found in sheet\")\n",
    "    \n",
    "    # Parse headers and data\n",
    "    headers = values[0]\n",
    "    data_rows = []\n",
    "    for i, row in enumerate(values[1:], start=1):\n",
    "        # Pad row to match header length\n",
    "        row_data = row + [''] * (len(headers) - len(row))\n",
    "        row_dict = {headers[j]: row_data[j] for j in range(len(headers))}\n",
    "        row_dict['_row_index'] = i + 1  # +1 for header row\n",
    "        data_rows.append(row_dict)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(data_rows)} sources\")\n",
    "    return sheets_service, headers, data_rows\n",
    "\n",
    "\n",
    "def update_sheet_row(sheets_service, row_index: int, headers: list, row_data: dict):\n",
    "    \"\"\"\n",
    "    Update MBFC columns in a specific row of the sheet.\n",
    "    \n",
    "    Args:\n",
    "        sheets_service: Google Sheets service instance\n",
    "        row_index: Row number in the sheet (1-indexed)\n",
    "        headers: List of column headers\n",
    "        row_data: Dictionary with column data including mbfc_bias and mbfc_factual\n",
    "    \"\"\"\n",
    "    # Find column indices\n",
    "    bias_col_idx = headers.index('mbfc_bias') if 'mbfc_bias' in headers else None\n",
    "    factual_col_idx = headers.index('mbfc_factual') if 'mbfc_factual' in headers else None\n",
    "    \n",
    "    if bias_col_idx is None or factual_col_idx is None:\n",
    "        raise ValueError(\"‚ùå Required columns 'mbfc_bias' and 'mbfc_factual' not found in sheet\")\n",
    "    \n",
    "    # Convert column index to letter (0->A, 1->B, etc.)\n",
    "    def col_to_letter(col_idx):\n",
    "        result = \"\"\n",
    "        while col_idx >= 0:\n",
    "            result = chr(65 + (col_idx % 26)) + result\n",
    "            col_idx = col_idx // 26 - 1\n",
    "        return result\n",
    "    \n",
    "    bias_col = col_to_letter(bias_col_idx)\n",
    "    factual_col = col_to_letter(factual_col_idx)\n",
    "    \n",
    "    # Update bias rating\n",
    "    if row_data.get('mbfc_bias'):\n",
    "        range_name = f\"main!{bias_col}{row_index}\"\n",
    "        body = {'values': [[row_data['mbfc_bias']]]}\n",
    "        sheets_service.spreadsheets().values().update(\n",
    "            spreadsheetId=SPREADSHEET_ID,\n",
    "            range=range_name,\n",
    "            valueInputOption='RAW',\n",
    "            body=body\n",
    "        ).execute()\n",
    "    \n",
    "    # Update factual rating\n",
    "    if row_data.get('mbfc_factual'):\n",
    "        range_name = f\"main!{factual_col}{row_index}\"\n",
    "        body = {'values': [[row_data['mbfc_factual']]]}\n",
    "        sheets_service.spreadsheets().values().update(\n",
    "            spreadsheetId=SPREADSHEET_ID,\n",
    "            range=range_name,\n",
    "            valueInputOption='RAW',\n",
    "            body=body\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mbfc_enrichment():\n",
    "    \"\"\"\n",
    "    Main workflow function that processes all sources and enriches them with MBFC data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load sheet data\n",
    "        sheets_service, headers, data_rows = load_sheet_data()\n",
    "        \n",
    "        # Verify required columns exist\n",
    "        if 'mbfc_bias' not in headers or 'mbfc_factual' not in headers:\n",
    "            print(\"‚ùå Error: Required columns 'mbfc_bias' and 'mbfc_factual' not found in sheet\")\n",
    "            print(f\"üìã Available columns: {', '.join(headers)}\")\n",
    "            return\n",
    "        \n",
    "        # Count existing vs needed enrichment\n",
    "        already_filled = sum(\n",
    "            1 for row in data_rows \n",
    "            if row.get('mbfc_bias', '').strip() and row.get('mbfc_factual', '').strip()\n",
    "        )\n",
    "        needs_enrichment = len(data_rows) - already_filled\n",
    "        \n",
    "        print(f\"üìä Status: {already_filled} already have MBFC data, {needs_enrichment} need enrichment\")\n",
    "        print(f\"üöÄ Starting MBFC enrichment...\\n\")\n",
    "        \n",
    "        # Process each row\n",
    "        start_time = time.time()\n",
    "        updated_count = 0\n",
    "        skipped_count = 0\n",
    "        not_found_count = 0\n",
    "        \n",
    "        for idx, row in enumerate(data_rows):\n",
    "            name = row.get('name', '').strip()\n",
    "            url = row.get('url', '').strip()\n",
    "            existing_bias = row.get('mbfc_bias', '').strip()\n",
    "            existing_factual = row.get('mbfc_factual', '').strip()\n",
    "            row_index = row.get('_row_index')\n",
    "            \n",
    "            # Skip rows with missing data\n",
    "            if not name or not url:\n",
    "                print(f\"‚è≠Ô∏è  [{idx + 1}/{len(data_rows)}] Skipping row {row_index}: missing name or URL\")\n",
    "                continue\n",
    "            \n",
    "            # Skip rows that already have both MBFC fields\n",
    "            if existing_bias and existing_factual:\n",
    "                print(f\"‚è≠Ô∏è  [{idx + 1}/{len(data_rows)}] Skipping {name}: already has MBFC data\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"üîç [{idx + 1}/{len(data_rows)}] Processing: {name}\")\n",
    "            print(f\"   URL: {url}\")\n",
    "            \n",
    "            # Fetch MBFC ratings\n",
    "            bias_rating, factual_rating = get_mbfc_ratings(name, url)\n",
    "            \n",
    "            if bias_rating or factual_rating:\n",
    "                # Update sheet with findings\n",
    "                row['mbfc_bias'] = bias_rating or \"\"\n",
    "                row['mbfc_factual'] = factual_rating or \"\"\n",
    "                \n",
    "                try:\n",
    "                    update_sheet_row(sheets_service, row_index, headers, row)\n",
    "                    updated_count += 1\n",
    "                    print(f\"   ‚úÖ Found: Bias={bias_rating}, Factual={factual_rating}\")\n",
    "                    print(f\"   üìù Updated sheet\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error updating sheet: {str(e)}\\n\")\n",
    "            else:\n",
    "                not_found_count += 1\n",
    "                print(f\"   ‚ùå Not found on MBFC\\n\")\n",
    "            \n",
    "            # Apply rate limiting\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "        \n",
    "        # Print summary\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä Summary\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"‚úÖ Sources updated: {updated_count}\")\n",
    "        print(f\"‚è≠Ô∏è  Sources skipped (already had data): {skipped_count}\")\n",
    "        print(f\"‚ùå Sources not found on MBFC: {not_found_count}\")\n",
    "        print(f\"‚è±Ô∏è  Total time elapsed: {elapsed/60:.1f} minutes\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the MBFC enrichment process\n",
    "process_mbfc_enrichment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
