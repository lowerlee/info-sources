{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# MBFC Enrichment Notebook\n",
    "\n",
    "This notebook enriches information sources with Media Bias Fact Check (MBFC) data by scraping bias, factual reporting, and credibility ratings.\n",
    "\n",
    "## Purpose\n",
    "Automatically fetch MBFC bias, factual, and credibility ratings for sources in the Google Sheet and update the sheet with this information.\n",
    "\n",
    "## Requirements\n",
    "- **Credentials**: `credentials.json` file in the root directory (Google service account)\n",
    "- **Dependencies**: beautifulsoup4, requests, google-api-python-client\n",
    "- **Sheet Columns**: The sheet must have `mbfc_bias`, `mbfc_factual`, and `mbfc_credibility_rating` columns\n",
    "\n",
    "## How it works\n",
    "1. Connects to Google Sheets and loads source data\n",
    "2. For each source without MBFC data:\n",
    "   - Searches for the source on mediabiasfactcheck.com\n",
    "   - Extracts bias rating, factual reporting rating, and credibility rating\n",
    "   - Updates the Google Sheet with the findings\n",
    "3. Applies rate limiting to avoid overwhelming MBFC servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import re\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SERVICE_ACCOUNT_FILE = \"/workspaces/info-sources/credentials.json\"\n",
    "SPREADSHEET_ID = \"1NywRL9IBR69R0eSrOE9T6mVUbfJHwaALL0vp2K0TLbY\"\n",
    "SHEET_RANGE = \"main!A:I\"\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "\n",
    "# MBFC Configuration\n",
    "MBFC_BASE_URL = \"https://mediabiasfactcheck.com/\"\n",
    "DELAY_BETWEEN_REQUESTS = 2.0  # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract domain name from URL and remove www prefix.\n",
    "    \n",
    "    Args:\n",
    "        url: Full URL string\n",
    "        \n",
    "    Returns:\n",
    "        Domain name without www prefix\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc or parsed.path\n",
    "        # Remove www prefix\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "        return domain\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def normalize_source_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize source name for comparison by removing special characters and extra spaces.\n",
    "    \n",
    "    Args:\n",
    "        name: Source name to normalize\n",
    "        \n",
    "    Returns:\n",
    "        Normalized source name in lowercase\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    normalized = name.lower().strip()\n",
    "    # Remove special characters except spaces and hyphens\n",
    "    normalized = re.sub(r'[^a-z0-9\\s-]', '', normalized)\n",
    "    # Normalize spaces\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def extract_mbfc_page_title(html_content: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the source name/title from an MBFC page.\n",
    "    \n",
    "    Args:\n",
    "        html_content: HTML content of the MBFC page\n",
    "        \n",
    "    Returns:\n",
    "        Source name as it appears on MBFC, or None if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Try to find the main heading (h1)\n",
    "        h1 = soup.find('h1', class_='page-title')\n",
    "        if h1:\n",
    "            return h1.get_text().strip()\n",
    "        \n",
    "        # Fallback: try any h1\n",
    "        h1 = soup.find('h1')\n",
    "        if h1:\n",
    "            return h1.get_text().strip()\n",
    "        \n",
    "        # Fallback: try page title\n",
    "        title = soup.find('title')\n",
    "        if title:\n",
    "            title_text = title.get_text().strip()\n",
    "            # Remove common suffixes from title\n",
    "            title_text = re.sub(r'\\s*-\\s*Media Bias/Fact Check.*$', '', title_text, flags=re.IGNORECASE)\n",
    "            return title_text.strip()\n",
    "        \n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def names_match(search_name: str, page_name: str, threshold: float = 0.7) -> bool:\n",
    "    \"\"\"\n",
    "    Check if two source names are similar enough to be considered a match.\n",
    "    \n",
    "    Args:\n",
    "        search_name: The name being searched for\n",
    "        page_name: The name found on the MBFC page\n",
    "        threshold: Similarity threshold (0-1), default 0.7\n",
    "        \n",
    "    Returns:\n",
    "        True if names match sufficiently, False otherwise\n",
    "    \"\"\"\n",
    "    # Normalize both names\n",
    "    norm_search = normalize_source_name(search_name)\n",
    "    norm_page = normalize_source_name(page_name)\n",
    "    \n",
    "    # Exact match after normalization\n",
    "    if norm_search == norm_page:\n",
    "        return True\n",
    "    \n",
    "    # Check if one is contained in the other (but not too different in length)\n",
    "    len_diff_ratio = abs(len(norm_search) - len(norm_page)) / max(len(norm_search), len(norm_page))\n",
    "    if len_diff_ratio < 0.3:  # Allow 30% length difference\n",
    "        if norm_search in norm_page or norm_page in norm_search:\n",
    "            return True\n",
    "    \n",
    "    # Simple word-based similarity\n",
    "    search_words = set(norm_search.split())\n",
    "    page_words = set(norm_page.split())\n",
    "    \n",
    "    # If search name is short (1-2 words), require exact match of all words\n",
    "    if len(search_words) <= 2:\n",
    "        return search_words == page_words\n",
    "    \n",
    "    # For longer names, use Jaccard similarity\n",
    "    if search_words and page_words:\n",
    "        intersection = search_words.intersection(page_words)\n",
    "        union = search_words.union(page_words)\n",
    "        similarity = len(intersection) / len(union)\n",
    "        return similarity >= threshold\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def search_mbfc(source_name: str, source_url: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Search for source on MBFC by trying different URL patterns.\n",
    "    Validates that the found page actually matches the searched source.\n",
    "    \n",
    "    Args:\n",
    "        source_name: Name of the source\n",
    "        source_url: URL of the source\n",
    "        \n",
    "    Returns:\n",
    "        MBFC page URL if found and validated, None otherwise\n",
    "    \"\"\"\n",
    "    # Convert source name to slug format (lowercase, replace spaces with hyphens)\n",
    "    name_slug = source_name.lower().strip()\n",
    "    name_slug = re.sub(r'[^a-z0-9\\s-]', '', name_slug)\n",
    "    name_slug = re.sub(r'\\s+', '-', name_slug)\n",
    "    name_slug = re.sub(r'-+', '-', name_slug)\n",
    "    \n",
    "    # Extract domain from URL\n",
    "    domain = extract_domain(source_url)\n",
    "    domain_slug = domain.replace('.', '-') if domain else \"\"\n",
    "    \n",
    "    # Try different URL patterns\n",
    "    patterns_to_try = []\n",
    "    if name_slug:\n",
    "        patterns_to_try.append(name_slug)\n",
    "    if domain_slug and domain_slug != name_slug:\n",
    "        patterns_to_try.append(domain_slug)\n",
    "    \n",
    "    for pattern in patterns_to_try:\n",
    "        try:\n",
    "            mbfc_url = f\"{MBFC_BASE_URL}{pattern}/\"\n",
    "            response = requests.get(mbfc_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            \n",
    "            if response.status_code == 200 and 'Bias Rating:' in response.text:\n",
    "                # Extract the page title/name to validate it matches\n",
    "                page_title = extract_mbfc_page_title(response.text)\n",
    "                \n",
    "                if page_title:\n",
    "                    # Check if the page title matches the source name we're searching for\n",
    "                    if names_match(source_name, page_title):\n",
    "                        return mbfc_url\n",
    "                    else:\n",
    "                        # Log the mismatch for debugging\n",
    "                        print(f\"   âš ï¸  Found MBFC page but name mismatch: '{source_name}' vs '{page_title}'\")\n",
    "                else:\n",
    "                    # If we can't extract the title, be conservative and skip\n",
    "                    print(f\"   âš ï¸  Found MBFC page but couldn't extract title for validation\")\n",
    "                    \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_mbfc_rating(rating: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean MBFC rating by removing numerical scores in parentheses.\n",
    "    \n",
    "    Args:\n",
    "        rating: Raw rating string from MBFC (e.g., \"HIGH (1.8)\" or \"VERY HIGH (0.0)\")\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned rating without scores (e.g., \"HIGH\" or \"VERY HIGH\")\n",
    "    \n",
    "    Examples:\n",
    "        >>> clean_mbfc_rating(\"HIGH (1.8)\")\n",
    "        \"HIGH\"\n",
    "        >>> clean_mbfc_rating(\"VERY HIGH (0.0)\")\n",
    "        \"VERY HIGH\"\n",
    "        >>> clean_mbfc_rating(\"MOSTLY FACTUAL\")\n",
    "        \"MOSTLY FACTUAL\"\n",
    "        >>> clean_mbfc_rating(\"LEFT-CENTER\")\n",
    "        \"LEFT-CENTER\"\n",
    "        >>> clean_mbfc_rating(\"RIGHT (7.1)\")\n",
    "        \"RIGHT\"\n",
    "    \"\"\"\n",
    "    if not rating:\n",
    "        return rating\n",
    "    \n",
    "    # Remove anything in parentheses along with the parentheses\n",
    "    # Pattern: \\s*\\([^)]*\\) matches optional space + opening paren + any chars + closing paren\n",
    "    cleaned = re.sub(r'\\s*\\([^)]*\\)', '', rating)\n",
    "    \n",
    "    # Remove extra whitespace and return\n",
    "    return ' '.join(cleaned.split()).strip()\n",
    "\n",
    "\n",
    "def extract_mbfc_data(mbfc_url: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Parse MBFC page HTML and extract bias, factual, and credibility ratings.\n",
    "    \n",
    "    Args:\n",
    "        mbfc_url: URL of the MBFC page\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (bias_rating, factual_rating, credibility_rating)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch the page\n",
    "        response = requests.get(mbfc_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        if response.status_code != 200:\n",
    "            return None, None, None\n",
    "        \n",
    "        # Parse with BeautifulSoup to strip HTML tags\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_text = soup.get_text()\n",
    "        \n",
    "        # Split into lines for line-by-line processing\n",
    "        lines = page_text.split('\\n')\n",
    "        \n",
    "        # Initialize return values\n",
    "        bias_rating = None\n",
    "        factual_rating = None\n",
    "        credibility_rating = None\n",
    "        \n",
    "        # Process each line to find our target fields\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Extract Bias Rating\n",
    "            if 'Bias Rating:' in line and not bias_rating:\n",
    "                if ':' in line:\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) > 1 and parts[1].strip():\n",
    "                        bias_rating = parts[1].strip()\n",
    "                    elif i + 1 < len(lines):\n",
    "                        bias_rating = lines[i + 1].strip()\n",
    "            \n",
    "            # Extract Factual Reporting\n",
    "            elif 'Factual Reporting:' in line and not factual_rating:\n",
    "                if ':' in line:\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) > 1 and parts[1].strip():\n",
    "                        factual_rating = parts[1].strip()\n",
    "                    elif i + 1 < len(lines):\n",
    "                        factual_rating = lines[i + 1].strip()\n",
    "            \n",
    "            # Extract MBFC Credibility Rating\n",
    "            elif 'MBFC Credibility Rating:' in line and not credibility_rating:\n",
    "                if ':' in line:\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) > 1 and parts[1].strip():\n",
    "                        credibility_rating = parts[1].strip()\n",
    "                    elif i + 1 < len(lines):\n",
    "                        credibility_rating = lines[i + 1].strip()\n",
    "            \n",
    "            # Alternative: Check for \"Credibility:\" without \"MBFC\" prefix\n",
    "            elif 'Credibility:' in line and 'MBFC' not in line and not credibility_rating:\n",
    "                if ':' in line:\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) > 1 and parts[1].strip():\n",
    "                        credibility_rating = parts[1].strip()\n",
    "                    elif i + 1 < len(lines):\n",
    "                        credibility_rating = lines[i + 1].strip()\n",
    "        \n",
    "        # Clean up extracted values - includes score removal and whitespace normalization\n",
    "        if bias_rating:\n",
    "            bias_rating = clean_mbfc_rating(bias_rating)\n",
    "        if factual_rating:\n",
    "            factual_rating = clean_mbfc_rating(factual_rating)\n",
    "        if credibility_rating:\n",
    "            credibility_rating = clean_mbfc_rating(credibility_rating)\n",
    "        \n",
    "        return bias_rating, factual_rating, credibility_rating\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ Error extracting data: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def get_mbfc_ratings(source_name: str, source_url: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Combine search and extraction to get MBFC ratings for a source.\n",
    "    \n",
    "    Args:\n",
    "        source_name: Name of the source\n",
    "        source_url: URL of the source\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (bias_rating, factual_rating, credibility_rating)\n",
    "    \"\"\"\n",
    "    mbfc_url = search_mbfc(source_name, source_url)\n",
    "    if mbfc_url:\n",
    "        return extract_mbfc_data(mbfc_url)\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sheet_data():\n",
    "    \"\"\"\n",
    "    Load data from Google Sheets.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (sheets_service, headers, data_rows)\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”— Connecting to Google Sheets...\")\n",
    "    creds = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE,\n",
    "        scopes=SCOPES\n",
    "    )\n",
    "    sheets_service = build(\"sheets\", \"v4\", credentials=creds)\n",
    "    print(\"âœ… Connected to Google Sheets\")\n",
    "    \n",
    "    print(\"ðŸ“‚ Loading data from Google Sheet...\")\n",
    "    sheet = sheets_service.spreadsheets()\n",
    "    result = sheet.values().get(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=SHEET_RANGE\n",
    "    ).execute()\n",
    "    \n",
    "    values = result.get(\"values\", [])\n",
    "    \n",
    "    if not values:\n",
    "        raise ValueError(\"âŒ No data found in sheet\")\n",
    "    \n",
    "    # Parse headers and data\n",
    "    headers = values[0]\n",
    "    data_rows = []\n",
    "    for i, row in enumerate(values[1:], start=1):\n",
    "        # Pad row to match header length\n",
    "        row_data = row + [''] * (len(headers) - len(row))\n",
    "        row_dict = {headers[j]: row_data[j] for j in range(len(headers))}\n",
    "        row_dict['_row_index'] = i + 1  # +1 for header row\n",
    "        data_rows.append(row_dict)\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(data_rows)} sources\")\n",
    "    return sheets_service, headers, data_rows\n",
    "\n",
    "\n",
    "def update_sheet_row(sheets_service, row_index: int, headers: list, row_data: dict):\n",
    "    \"\"\"\n",
    "    Update MBFC columns in a specific row of the sheet.\n",
    "    \n",
    "    Args:\n",
    "        sheets_service: Google Sheets service instance\n",
    "        row_index: Row number in the sheet (1-indexed)\n",
    "        headers: List of column headers\n",
    "        row_data: Dictionary with column data including mbfc_bias, mbfc_factual, and mbfc_credibility_rating\n",
    "    \"\"\"\n",
    "    # Find column indices\n",
    "    bias_col_idx = headers.index('mbfc_bias') if 'mbfc_bias' in headers else None\n",
    "    factual_col_idx = headers.index('mbfc_factual') if 'mbfc_factual' in headers else None\n",
    "    credibility_col_idx = headers.index('mbfc_credibility_rating') if 'mbfc_credibility_rating' in headers else None\n",
    "    \n",
    "    if bias_col_idx is None or factual_col_idx is None or credibility_col_idx is None:\n",
    "        raise ValueError(\"âŒ Required columns 'mbfc_bias', 'mbfc_factual', and 'mbfc_credibility_rating' not found in sheet\")\n",
    "    \n",
    "    # Convert column index to letter (0->A, 1->B, etc.)\n",
    "    def col_to_letter(col_idx):\n",
    "        result = \"\"\n",
    "        while col_idx >= 0:\n",
    "            result = chr(65 + (col_idx % 26)) + result\n",
    "            col_idx = col_idx // 26 - 1\n",
    "        return result\n",
    "    \n",
    "    bias_col = col_to_letter(bias_col_idx)\n",
    "    factual_col = col_to_letter(factual_col_idx)\n",
    "    credibility_col = col_to_letter(credibility_col_idx)\n",
    "    \n",
    "    # Update bias rating\n",
    "    if row_data.get('mbfc_bias'):\n",
    "        range_name = f\"main!{bias_col}{row_index}\"\n",
    "        body = {'values': [[row_data['mbfc_bias']]]}\n",
    "        sheets_service.spreadsheets().values().update(\n",
    "            spreadsheetId=SPREADSHEET_ID,\n",
    "            range=range_name,\n",
    "            valueInputOption='RAW',\n",
    "            body=body\n",
    "        ).execute()\n",
    "    \n",
    "    # Update factual rating\n",
    "    if row_data.get('mbfc_factual'):\n",
    "        range_name = f\"main!{factual_col}{row_index}\"\n",
    "        body = {'values': [[row_data['mbfc_factual']]]}\n",
    "        sheets_service.spreadsheets().values().update(\n",
    "            spreadsheetId=SPREADSHEET_ID,\n",
    "            range=range_name,\n",
    "            valueInputOption='RAW',\n",
    "            body=body\n",
    "        ).execute()\n",
    "    \n",
    "    # Update credibility rating\n",
    "    if row_data.get('mbfc_credibility_rating'):\n",
    "        range_name = f\"main!{credibility_col}{row_index}\"\n",
    "        body = {'values': [[row_data['mbfc_credibility_rating']]]}\n",
    "        sheets_service.spreadsheets().values().update(\n",
    "            spreadsheetId=SPREADSHEET_ID,\n",
    "            range=range_name,\n",
    "            valueInputOption='RAW',\n",
    "            body=body\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mbfc_enrichment():\n",
    "    \"\"\"\n",
    "    Main workflow function that processes all sources and enriches them with MBFC data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load sheet data\n",
    "        sheets_service, headers, data_rows = load_sheet_data()\n",
    "        \n",
    "        # Verify required columns exist\n",
    "        if 'mbfc_bias' not in headers or 'mbfc_factual' not in headers or 'mbfc_credibility_rating' not in headers:\n",
    "            print(\"âŒ Error: Required columns 'mbfc_bias', 'mbfc_factual', and 'mbfc_credibility_rating' not found in sheet\")\n",
    "            print(f\"ðŸ“‹ Available columns: {', '.join(headers)}\")\n",
    "            return\n",
    "        \n",
    "        # Count existing vs needed enrichment\n",
    "        already_filled = sum(\n",
    "            1 for row in data_rows \n",
    "            if row.get('mbfc_bias', '').strip() and row.get('mbfc_factual', '').strip() and row.get('mbfc_credibility_rating', '').strip()\n",
    "        )\n",
    "        needs_enrichment = len(data_rows) - already_filled\n",
    "        \n",
    "        print(f\"ðŸ“Š Status: {already_filled} already have MBFC data, {needs_enrichment} need enrichment\")\n",
    "        print(f\"ðŸš€ Starting MBFC enrichment...\\n\")\n",
    "        \n",
    "        # Process each row\n",
    "        start_time = time.time()\n",
    "        updated_count = 0\n",
    "        cleaned_count = 0\n",
    "        skipped_count = 0\n",
    "        not_found_count = 0\n",
    "        \n",
    "        for idx, row in enumerate(data_rows):\n",
    "            name = row.get('name', '').strip()\n",
    "            url = row.get('url', '').strip()\n",
    "            existing_bias = row.get('mbfc_bias', '').strip()\n",
    "            existing_factual = row.get('mbfc_factual', '').strip()\n",
    "            existing_credibility = row.get('mbfc_credibility_rating', '').strip()\n",
    "            row_index = row.get('_row_index')\n",
    "            \n",
    "            # Skip rows with missing data\n",
    "            if not name or not url:\n",
    "                print(f\"â­ï¸  [{idx + 1}/{len(data_rows)}] Skipping row {row_index}: missing name or URL\")\n",
    "                continue\n",
    "            \n",
    "            # Clean existing values during processing to remove any scores\n",
    "            cleaned_bias = clean_mbfc_rating(existing_bias) if existing_bias else \"\"\n",
    "            cleaned_factual = clean_mbfc_rating(existing_factual) if existing_factual else \"\"\n",
    "            cleaned_credibility = clean_mbfc_rating(existing_credibility) if existing_credibility else \"\"\n",
    "            \n",
    "            # Check if any existing value needs cleaning (has changed after cleanup)\n",
    "            needs_cleaning = (\n",
    "                (existing_bias and cleaned_bias != existing_bias) or\n",
    "                (existing_factual and cleaned_factual != existing_factual) or\n",
    "                (existing_credibility and cleaned_credibility != existing_credibility)\n",
    "            )\n",
    "            \n",
    "            # If values need cleaning, update them\n",
    "            if needs_cleaning:\n",
    "                print(f\"ðŸ§¹ [{idx + 1}/{len(data_rows)}] Cleaning scores from {name}\")\n",
    "                if existing_bias != cleaned_bias:\n",
    "                    print(f\"   Bias: '{existing_bias}' â†’ '{cleaned_bias}'\")\n",
    "                if existing_factual != cleaned_factual:\n",
    "                    print(f\"   Factual: '{existing_factual}' â†’ '{cleaned_factual}'\")\n",
    "                if existing_credibility != cleaned_credibility:\n",
    "                    print(f\"   Credibility: '{existing_credibility}' â†’ '{cleaned_credibility}'\")\n",
    "                \n",
    "                row['mbfc_bias'] = cleaned_bias\n",
    "                row['mbfc_factual'] = cleaned_factual\n",
    "                row['mbfc_credibility_rating'] = cleaned_credibility\n",
    "                \n",
    "                try:\n",
    "                    update_sheet_row(sheets_service, row_index, headers, row)\n",
    "                    cleaned_count += 1\n",
    "                    print(f\"   âœ… Cleaned and updated sheet\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Error updating sheet: {str(e)}\\n\")\n",
    "                \n",
    "                # Apply rate limiting after update\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "                continue\n",
    "            \n",
    "            # Skip rows that already have all three MBFC fields (and don't need cleaning)\n",
    "            if cleaned_bias and cleaned_factual and cleaned_credibility:\n",
    "                print(f\"â­ï¸  [{idx + 1}/{len(data_rows)}] Skipping {name}: already has MBFC data\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"ðŸ” [{idx + 1}/{len(data_rows)}] Processing: {name}\")\n",
    "            print(f\"   URL: {url}\")\n",
    "            \n",
    "            # Fetch MBFC ratings\n",
    "            bias_rating, factual_rating, credibility_rating = get_mbfc_ratings(name, url)\n",
    "            \n",
    "            if bias_rating or factual_rating or credibility_rating:\n",
    "                # Update sheet with findings\n",
    "                row['mbfc_bias'] = bias_rating or \"\"\n",
    "                row['mbfc_factual'] = factual_rating or \"\"\n",
    "                row['mbfc_credibility_rating'] = credibility_rating or \"\"\n",
    "                \n",
    "                try:\n",
    "                    update_sheet_row(sheets_service, row_index, headers, row)\n",
    "                    updated_count += 1\n",
    "                    print(f\"   âœ… Found: Bias={bias_rating}, Factual={factual_rating}, Credibility={credibility_rating}\")\n",
    "                    print(f\"   ðŸ“ Updated sheet\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Error updating sheet: {str(e)}\\n\")\n",
    "            else:\n",
    "                not_found_count += 1\n",
    "                print(f\"   âŒ Not found on MBFC\\n\")\n",
    "            \n",
    "            # Apply rate limiting\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "        \n",
    "        # Print summary\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸ“Š Summary\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"âœ… Sources updated with new MBFC data: {updated_count}\")\n",
    "        print(f\"ðŸ§¹ Sources cleaned (scores removed): {cleaned_count}\")\n",
    "        print(f\"â­ï¸  Sources skipped (already had data): {skipped_count}\")\n",
    "        print(f\"âŒ Sources not found on MBFC: {not_found_count}\")\n",
    "        print(f\"â±ï¸  Total time elapsed: {elapsed/60:.1f} minutes\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Connecting to Google Sheets...\n",
      "âœ… Connected to Google Sheets\n",
      "ðŸ“‚ Loading data from Google Sheet...\n",
      "âœ… Loaded 364 sources\n",
      "ðŸ“Š Status: 0 already have MBFC data, 364 need enrichment\n",
      "ðŸš€ Starting MBFC enrichment...\n",
      "\n",
      "ðŸ” [1/364] Processing: Insight Crime\n",
      "   URL: https://insightcrime.org/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Insight Crime' vs 'InSight Crime â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [2/364] Processing: United States Institute of Peace\n",
      "   URL: https://www.usip.org/publications\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'United States Institute of Peace' vs 'United States Institute of Peace (USIP) â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [3/364] Processing: Pew Research\n",
      "   URL: https://www.pewresearch.org/publications/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Pew Research' vs 'Pew Research â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [4/364] Processing: ProPublica\n",
      "   URL: https://www.propublica.org/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'ProPublica' vs 'Propublica â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [5/364] Processing: Associated Press\n",
      "   URL: https://apnews.com/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Associated Press' vs 'Associated Press (AP News) â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [6/364] Processing: Reuters\n",
      "   URL: https://www.reuters.com/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Reuters' vs 'Reuters â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [7/364] Processing: Al Jazeera\n",
      "   URL: https://www.aljazeera.com/news/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Al Jazeera' vs 'Al Jazeera â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [8/364] Processing: Brookings Institute\n",
      "   URL: https://www.brookings.edu/research-commentary/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Brookings Institute' vs 'Brookings Institution â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [9/364] Processing: Business & Human Rights Resource Center\n",
      "   URL: https://www.business-humanrights.org/en/latest-news/?&language=en\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [10/364] Processing: Armed Conflict Location & Event Data Project\n",
      "   URL: https://acleddata.com/\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [11/364] Processing: Institute for the Study of War\n",
      "   URL: https://www.understandingwar.org/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Institute for the Study of War' vs 'Institute for the Study of War (ISW) â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [12/364] Processing: Carnegie Endowment For International Peace\n",
      "   URL: https://carnegieendowment.org/research?lang=en\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Carnegie Endowment For International Peace' vs 'Carnegie Endowment for International Peace (CEIP) â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [13/364] Processing: Center for Strategic and International Studies\n",
      "   URL: https://www.csis.org/analysis\n",
      "   âœ… Found: Bias=LEAST BIASED, Factual=VERY HIGH, Credibility=HIGH CREDIBILITY\n",
      "   ðŸ“ Updated sheet\n",
      "\n",
      "ðŸ” [14/364] Processing: Committee for a Responsible Federal Budget\n",
      "   URL: https://www.crfb.org/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Committee for a Responsible Federal Budget' vs 'Committee for a Responsible Federal Budget â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [15/364] Processing: National Security Archive\n",
      "   URL: https://nsarchive.gwu.edu/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'National Security Archive' vs 'National Security Archive (NSA) â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [16/364] Processing: The Marshall Project\n",
      "   URL: https://www.themarshallproject.org/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'The Marshall Project' vs 'The Marshall Project â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [17/364] Processing: The Southern Poverty Law Center\n",
      "   URL: https://www.splcenter.org/\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [18/364] Processing: Carnegie Middle East Center\n",
      "   URL: https://carnegieendowment.org/middle-east?lang=en\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [19/364] Processing: AP-NORC\n",
      "   URL: https://apnorc.org/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'AP-NORC' vs 'AP-NORC- Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [20/364] Processing: Lawfare\n",
      "   URL: https://www.lawfaremedia.org/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Lawfare' vs 'Lawfare Blog â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [21/364] Processing: The National Academies Press\n",
      "   URL: https://nap.nationalacademies.org/\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [22/364] Processing: Transportation Research Board\n",
      "   URL: https://www.trb.org/Publications/PubsTRBPublicationsbySeries.aspx\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [23/364] Processing: American Institute in Taiwan\n",
      "   URL: https://www.ait.org.tw/\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [24/364] Processing: Heritage Foundation\n",
      "   URL: https://www.heritage.org/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Heritage Foundation' vs 'Heritage Foundation â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [25/364] Processing: Migration Policy Institute\n",
      "   URL: https://www.migrationpolicy.org/research\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Migration Policy Institute' vs 'Migration Policy Institute â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [26/364] Processing: OVD-Info\n",
      "   URL: https://en.ovdinfo.org/\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [27/364] Processing: Peter G. Peterson Foundation\n",
      "   URL: https://www.pgpf.org/the-latest\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [28/364] Processing: Center for Federal Tax Policy\n",
      "   URL: https://taxfoundation.org/research/federal-tax/\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [29/364] Processing: Center for State Tax Policy\n",
      "   URL: https://taxfoundation.org/research/state-tax/\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [30/364] Processing: CATO Institute\n",
      "   URL: https://www.cato.org/search/category/study\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'CATO Institute' vs 'Cato Institute â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [31/364] Processing: US Customs and Border Protection\n",
      "   URL: https://www.cbp.gov/newsroom\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [32/364] Processing: Tel Aviv University: Center for Security Studies\n",
      "   URL: https://www.inss.org.il/publication/\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [33/364] Processing: Asharq Al-Awsat\n",
      "   URL: https://aawsat.com/\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [34/364] Processing: Africa Center for Strategic Studies\n",
      "   URL: https://africacenter.org/\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [35/364] Processing: Institute on Taxation and Economic Policy\n",
      "   URL: https://itep.org/category/publications/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Institute on Taxation and Economic Policy' vs 'Institute on Taxation and Economic Policy (ITEP) â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [36/364] Processing: Economic Policy Institute\n",
      "   URL: https://www.epi.org/publications/\n",
      "   âš ï¸  Found MBFC page but name mismatch: 'Economic Policy Institute' vs 'Economic Policy Institute â€“ Bias and Credibility'\n",
      "   âŒ Not found on MBFC\n",
      "\n",
      "ðŸ” [37/364] Processing: Voice of Belarus\n",
      "   URL: https://www.voiceofbelarus.org/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Execute the MBFC enrichment process\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mprocess_mbfc_enrichment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mprocess_mbfc_enrichment\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Fetch MBFC ratings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m bias_rating, factual_rating, credibility_rating = \u001b[43mget_mbfc_ratings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bias_rating \u001b[38;5;129;01mor\u001b[39;00m factual_rating \u001b[38;5;129;01mor\u001b[39;00m credibility_rating:\n\u001b[32m     95\u001b[39m     \u001b[38;5;66;03m# Update sheet with findings\u001b[39;00m\n\u001b[32m     96\u001b[39m     row[\u001b[33m'\u001b[39m\u001b[33mmbfc_bias\u001b[39m\u001b[33m'\u001b[39m] = bias_rating \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 303\u001b[39m, in \u001b[36mget_mbfc_ratings\u001b[39m\u001b[34m(source_name, source_url)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_mbfc_ratings\u001b[39m(source_name: \u001b[38;5;28mstr\u001b[39m, source_url: \u001b[38;5;28mstr\u001b[39m) -> Tuple[Optional[\u001b[38;5;28mstr\u001b[39m], Optional[\u001b[38;5;28mstr\u001b[39m], Optional[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m    293\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m    Combine search and extraction to get MBFC ratings for a source.\u001b[39;00m\n\u001b[32m    295\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    301\u001b[39m \u001b[33;03m        Tuple of (bias_rating, factual_rating, credibility_rating)\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     mbfc_url = \u001b[43msearch_mbfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mbfc_url:\n\u001b[32m    305\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m extract_mbfc_data(mbfc_url)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36msearch_mbfc\u001b[39m\u001b[34m(source_name, source_url)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    152\u001b[39m     mbfc_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMBFC_BASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmbfc_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mUser-Agent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMozilla/5.0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mBias Rating:\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response.text:\n\u001b[32m    156\u001b[39m         \u001b[38;5;66;03m# Extract the page title/name to validate it matches\u001b[39;00m\n\u001b[32m    157\u001b[39m         page_title = extract_mbfc_page_title(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/urllib3/connection.py:571\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    568\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    570\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    574\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Execute the MBFC enrichment process\n",
    "process_mbfc_enrichment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Info Source",
   "language": "python",
   "name": "profit-status-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
