{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPC Hybrid Keyword + Embedding Analysis\n",
    "## GDELT-Style Two-Stage Article Analysis\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Keyword Filtering** (Stage 1) - Like GDELT's GKG filtering\n",
    "2. **Semantic Analysis** (Stage 2) - Like GDELT's GSG embeddings\n",
    "3. **Combined workflows** for powerful article discovery and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "from bpc_hybrid_analysis import BPCHybridSearch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Initialize\n",
    "searcher = BPCHybridSearch(embeddings_path='../data/bpc_embeddings.pkl')\n",
    "\n",
    "print(f\"Loaded {len(searcher.embeddings_data)} articles\")\n",
    "print(f\"Embedding dimensions: {searcher.embeddings_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore Your Data\n",
    "### See available tags and policy areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique values\n",
    "all_tags = set()\n",
    "all_policy_areas = set()\n",
    "all_people = set()\n",
    "\n",
    "for item in searcher.embeddings_data:\n",
    "    all_tags.update(item.get('tags', []))\n",
    "    all_policy_areas.update(item.get('policy_areas', []))\n",
    "    all_people.update(item.get('related_people', []))\n",
    "\n",
    "print(f\"Total unique tags: {len(all_tags)}\")\n",
    "print(f\"\\nTop 20 tags:\")\n",
    "tag_counts = {}\n",
    "for item in searcher.embeddings_data:\n",
    "    for tag in item.get('tags', []):\n",
    "        tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
    "\n",
    "for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"  {tag}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal policy areas: {len(all_policy_areas)}\")\n",
    "print(f\"Policy areas: {sorted(all_policy_areas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Two-Stage Analysis: Keyword Filter + Semantic Clustering\n",
    "### Example: Analyzing AI/Technology articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Filter by keywords\n",
    "tech_indices = searcher.keyword_filter(\n",
    "    tags=['Technology', 'Artificial Intelligence']  # Adjust based on your actual tags\n",
    ")\n",
    "\n",
    "print(f\"Found {len(tech_indices)} technology articles\")\n",
    "\n",
    "# Stage 2: Cluster semantically\n",
    "n_clusters = 5\n",
    "clusters = searcher.cluster_articles(tech_indices, n_clusters=n_clusters)\n",
    "\n",
    "# Display clusters\n",
    "for cluster_id, articles in clusters.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTER {cluster_id} ({len(articles)} articles)\")\n",
    "    print('='*60)\n",
    "    for i, article in enumerate(articles[:5], 1):  # Show top 5\n",
    "        print(f\"{i}. {article['title']}\")\n",
    "        print(f\"   Date: {article['date']}\")\n",
    "        print(f\"   Policy Areas: {', '.join(article.get('policy_areas', []))}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic Search Within Filtered Set\n",
    "### Example: Find articles about specific topics within a policy area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Filter to a policy area\n",
    "health_indices = searcher.keyword_filter(\n",
    "    policy_areas=['Health']\n",
    ")\n",
    "\n",
    "print(f\"Searching within {len(health_indices)} health policy articles...\\n\")\n",
    "\n",
    "# Stage 2: Semantic search for specific concept\n",
    "query = \"mental health crisis response and suicide prevention\"\n",
    "results = searcher.semantic_search(query, health_indices, top_k=10)\n",
    "\n",
    "print(f\"Top 10 matches for: '{query}'\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['title']}\")\n",
    "    print(f\"   Similarity: {result['similarity_score']:.3f}\")\n",
    "    print(f\"   Date: {result['date']}\")\n",
    "    print(f\"   URL: {result['url']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. \"More Like This\" Analysis\n",
    "### Find similar articles to a given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an article (use any URL from your dataset)\n",
    "target_url = searcher.embeddings_data[0]['url']  # Example: first article\n",
    "target_title = searcher.embeddings_data[0]['title']\n",
    "\n",
    "print(f\"Finding articles similar to:\\n'{target_title}'\\n\")\n",
    "\n",
    "# Find similar articles\n",
    "similar = searcher.find_similar_articles(target_url, top_k=10)\n",
    "\n",
    "print(\"Most similar articles:\")\n",
    "for i, result in enumerate(similar, 1):\n",
    "    print(f\"\\n{i}. {result['title']}\")\n",
    "    print(f\"   Similarity: {result['similarity_score']:.3f}\")\n",
    "    print(f\"   Date: {result['date']}\")\n",
    "    print(f\"   Policy: {', '.join(result.get('policy_areas', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Article Landscape\n",
    "### Use t-SNE to create 2D visualization (like GDELT's visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a subset for visualization (all articles or filtered)\n",
    "# For speed, let's use a sample if you have > 500 articles\n",
    "vis_indices = list(range(min(500, len(searcher.embeddings_data))))\n",
    "vis_embeddings = searcher.embeddings_matrix[vis_indices]\n",
    "\n",
    "# Reduce to 2D using t-SNE\n",
    "print(\"Running t-SNE dimensionality reduction...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(vis_embeddings)\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_data = []\n",
    "for i, idx in enumerate(vis_indices):\n",
    "    item = searcher.embeddings_data[idx]\n",
    "    plot_data.append({\n",
    "        'x': embeddings_2d[i, 0],\n",
    "        'y': embeddings_2d[i, 1],\n",
    "        'title': item['title'][:50],\n",
    "        'policy_area': ', '.join(item.get('policy_areas', ['Other']))[:30],\n",
    "        'date': item.get('date', ''),\n",
    "        'url': item.get('url', '')\n",
    "    })\n",
    "\n",
    "df_plot = pd.DataFrame(plot_data)\n",
    "\n",
    "# Create interactive plot\n",
    "fig = px.scatter(\n",
    "    df_plot, \n",
    "    x='x', \n",
    "    y='y',\n",
    "    color='policy_area',\n",
    "    hover_data=['title', 'date'],\n",
    "    title='BPC Article Landscape (Semantic Embeddings)',\n",
    "    width=1000,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=8, opacity=0.7))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced: Compare Policy Areas Semantically\n",
    "### Analyze how different policy areas cluster together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average embedding for each policy area\n",
    "policy_area_embeddings = {}\n",
    "\n",
    "for item in searcher.embeddings_data:\n",
    "    for area in item.get('policy_areas', []):\n",
    "        if area not in policy_area_embeddings:\n",
    "            policy_area_embeddings[area] = []\n",
    "        policy_area_embeddings[area].append(item['embedding'])\n",
    "\n",
    "# Calculate mean embeddings\n",
    "area_means = {}\n",
    "for area, embeddings in policy_area_embeddings.items():\n",
    "    if len(embeddings) >= 5:  # Only areas with 5+ articles\n",
    "        area_means[area] = np.mean(embeddings, axis=0)\n",
    "\n",
    "print(f\"Analyzing {len(area_means)} major policy areas\")\n",
    "\n",
    "# Compute similarity matrix between policy areas\n",
    "areas = list(area_means.keys())\n",
    "area_matrix = np.array([area_means[area] for area in areas])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_matrix = cosine_similarity(area_matrix)\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    xticklabels=areas,\n",
    "    yticklabels=areas,\n",
    "    cmap='YlOrRd',\n",
    "    annot=False,\n",
    "    square=True\n",
    ")\n",
    "plt.title('Semantic Similarity Between Policy Areas')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most/least similar policy area pairs\n",
    "print(\"\\nMost semantically similar policy areas:\")\n",
    "pairs = []\n",
    "for i in range(len(areas)):\n",
    "    for j in range(i+1, len(areas)):\n",
    "        pairs.append((areas[i], areas[j], similarity_matrix[i, j]))\n",
    "\n",
    "for area1, area2, sim in sorted(pairs, key=lambda x: x[2], reverse=True)[:10]:\n",
    "    print(f\"  {area1} â†” {area2}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results\n",
    "### Save filtered/analyzed results for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Export a filtered set with similarity scores\n",
    "query = \"climate change and energy transition\"\n",
    "energy_indices = searcher.keyword_filter(policy_areas=['Energy'])\n",
    "results = searcher.semantic_search(query, energy_indices, top_k=50)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame([{\n",
    "    'title': r['title'],\n",
    "    'date': r['date'],\n",
    "    'url': r['url'],\n",
    "    'similarity_score': r['similarity_score'],\n",
    "    'policy_areas': ', '.join(r.get('policy_areas', [])),\n",
    "    'tags': ', '.join(r.get('tags', []))\n",
    "} for r in results])\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv('../data/energy_climate_results.csv', index=False)\n",
    "print(f\"Saved {len(df_results)} results to energy_climate_results.csv\")\n",
    "df_results.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
